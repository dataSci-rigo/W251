ibmcloud sl vs create --datacenter=lon04 --hostname=v100a --domain=rego.com --image=2263543 --billing=hourly  --network 1000 --key=1474943 --flavor AC2_16X120X100  --san --disk 2TB

ibmcloud sl vs create --datacenter=lon04 --hostname=v100b --domain=rego.com --image=2263543 --billing=hourly  --network 1000 --key=1474943 --flavor AC2_16X120X100 --san --disk 2TB


mpirun -n 2 -H <vm1 private ip address>,<vm2 private ip address> --allow-run-as-root hostname


v100a.rego.com (Virtual Server)
Public IP: 158.175.139.203 (London 4)
Private IP: 10.45.92.78

v100b.rego.com (Virtual Server)
Public IP: 158.175.139.205 (London 4)
Private IP: 10.45.92.90





mpirun -n 2 -H 10.45.92.78,10.45.92.90 --allow-run-as-root hostname


mpirun -n 2 -H 10.45.92.90,10.45.92.78 --allow-run-as-root hostname

nohup mpirun --allow-run-as-root -n 4 -H 10.45.92.78:2,10.45.92.90:2 -bind-to none -map-by slot --mca btl_tcp_if_include eth0 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH python run.py --config_file=/data/transformer-base.py --use_horovod=True --mode=train_eval &

nohup mpirun --allow-run-as-root -n 4 -H 10.45.92.90:2,10.45.92.78:2 -bind-to none -map-by slot --mca btl_tcp_if_include eth0 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH python run.py --config_file=/data/transformer-base.py --use_horovod=True --mode=train_eval &

http://158.175.139.203:6006

tail -f nohup.out    

nohup tensorboard --logdir=/data/en-de-transformer



2019-07-05 05:00:58.446159: W tensorflow/core/kernels/data/cache_dataset_ops.cc:810] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the datasetwill be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2019-07-05 05:00:58.612820: W tensorflow/core/kernels/data/cache_dataset_ops.cc:810] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the datasetwill be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2019-07-05 05:01:02.229283: W tensorflow/core/kernels/data/cache_dataset_ops.cc:810] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the datasetwill be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
*** Finished training
*** Not enough steps for benchmarking

